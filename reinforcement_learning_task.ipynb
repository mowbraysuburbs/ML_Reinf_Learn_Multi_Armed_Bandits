{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Task\n",
    "\n",
    "For this task you will be required to solve a classic reinforcement learning problem: the [Multi-Armed Bandit problem](https://en.wikipedia.org/wiki/Multi-armed_bandit). \n",
    "\n",
    "In order to understand the Multi-Armed Bandit problem, imagine that you are in a casio facing multiple slot machines (let's say 10) in a row. Each of these slot machines allow you to play for free and has a maximum payout of 10 dollars. This means that each slot machine is guaranteed to give you a reward between 0 and 10 dollars. Each slot machine has a different average payout, and you have to figure out which one gives the most average reward so that you can maximise your reward in the shortest time possible. \n",
    "\n",
    "One way to do this would be to favour machines that had a good history of rewards relative to the number of times you pulled that machine. Chances are, however, that you wouldn’t just stick to one machine. You would probably also pull each of the machines a couple of times to get an idea of how each one behaved. In other words, would use a strategy  that is a mix of:\n",
    "\n",
    "* Exploring: trying out different machines; after all, how are you supposed to know which machine is the “best” if you don’t give each one a shot?\n",
    "\n",
    "* Exploiting: given the history of each machine, maximise your profit by picking the one with the best proportion of rewards to pulls.\n",
    "\n",
    "Choosing the right mix of exploration vs. exploitation is a difficult balance to achieve. Exploit too much, and you might miss out on the real best machine. Explore too much, and you’ll waste turns on subpar machines.\n",
    "\n",
    "\n",
    "## ϵ (epsilon)-greedy algorithm\n",
    "\n",
    "Different algorithms aimed at solving the Multi-Armed Bandit problem go about balancing exploration and exploitation in different ways. A famous approach to solving this problem is the ϵ (epsilon)-greedy algorithm. In \"greedy\" experiments, the lever with highest known payout is always pulled except when a random action is taken. A randomly chosen arm is pulled a fraction ϵ of the time. The other 1-ϵ of the time, the arm with highest known payout is pulled.\n",
    "\n",
    "The epsilon greedy strategy, essentially leaves this problem up to the user to solve by having them define a constant ϵ . ϵ  is then used by the algorithm in the following way:\n",
    "\n",
    "* Choose a random machine to pull with probability = ϵ.\n",
    "* Choose the best machine to pull with probability = 1 - ϵ.\n",
    "\n",
    "The algorithm defines the \"best\" machine very simply; it is just the one with the highest experimental mean. The experimental mean is calculated as the sum of the rewards from that machine divided by the number of times that machine has been pulled.\n",
    "\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Complete the code below.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "As always, we firstly need to import the necessary libraries and modules required to implement the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we will be solving the 10-armed bandit problem. We therefore create a numpy array, arms, of length n filled with random floats that can be understood as probabilities of action of that arm. We choose an epsilon value of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69590577, 0.58089898, 0.11775736, 0.85438027, 0.05261239,\n",
       "       0.09747499, 0.68334027, 0.03327031, 0.19575083, 0.89464735])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "arms = np.random.rand(n)\n",
    "eps = 0.1 # probability of exploration action\n",
    "arms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create a reward function. In this function, for each arm, you need to run a loop of 10 iterations, and generate a random float every time. If this random number is less than the probability of that arm, you'll add a 1 to the reward. After all iterations, you should have a value between 0 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "def reward(prob):\n",
    "    reward = 0\n",
    "    # Complete this functio\n",
    "    \n",
    "    for i in range(10):\n",
    "        outcome = np.random.rand(1)\n",
    "        if prob > outcome:\n",
    "            reward += 1\n",
    "        \n",
    "    return reward\n",
    "\n",
    "print(reward(arms[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initialise a memory array which has 1 row defaulted to random action index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av = np.array([np.random.randint(0,(n+1)), 0]).reshape(1,2) #av = action-value\n",
    "av.shape\n",
    "av[0][0]\n",
    "\n",
    "# print(reward(av[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the bestArm function. This function is your greedy strategy of choosing the best arm so far. This function accepts a memory array that stores the history of all actions and their rewards. It is a 2 x k matrix where each row is an index reference to your arms array (1st element), and the reward received (2nd element). For example, if a row in your memory array is [2, 8], it means that action 2 was taken (the 3rd element in our arms array) and you received a reward of 8 for taking that action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "\n",
    "shape = np.array(a)\n",
    "\n",
    "# len(shape[0])\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list = np.array([[2,8],[0,1],[8,8],[8,9],[3,9]])\n",
    "list = ([[2,8],[0,1],[1,8],[8,7],[3,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "1\n",
      "8\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "clm = np.array(list)\n",
    "for i in range(clm.shape[0]):\n",
    "    print(clm[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestArm(a):\n",
    "    \n",
    "    clm = np.array(a)\n",
    "    bestArm = 0 #default to 0\n",
    "    bestMean = 0\n",
    "    \n",
    "    for i in range(len(arms)):\n",
    "        plays = 0\n",
    "        result = 0\n",
    "        for j in range(clm.shape[0]):\n",
    "            if clm[j][0] == i+1:\n",
    "                plays += 1\n",
    "                result += clm[j][1]\n",
    "                \n",
    "        if plays > 0:\n",
    "            Mean = result/plays\n",
    "\n",
    "            if Mean > bestMean:\n",
    "                bestMean = Mean\n",
    "                bestArm = int(i)+1\n",
    "\n",
    "    mean.append(bestMean)\n",
    "    arm.append(bestArm)\n",
    "    a.append([bestArm, bestMean])\n",
    "    return bestArm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "list = ([[2,8],[0,1],[4,8],[8,7],[3,9]])\n",
    "print(bestArm(list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can create the main loop for each play. Let's play it 500 times and display a matplotlib scatter plot of the mean reward against the number of times the game is played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "8 10\n",
      "6 10\n",
      "8 10\n",
      "1 10\n",
      "0 0\n",
      "4 10\n",
      "3 10\n",
      "2 10\n",
      "4 10\n",
      "2 10\n",
      "3 10\n",
      "9 10\n",
      "0 0\n",
      "8 10\n",
      "8 10\n",
      "1 10\n",
      "2 10\n",
      "6 10\n",
      "7 10\n",
      "7 10\n",
      "4 10\n",
      "3 10\n",
      "9 10\n",
      "7 10\n",
      "9 10\n",
      "9 10\n",
      "10 10\n",
      "8 10\n",
      "3 10\n",
      "4 10\n",
      "9 10\n",
      "1 10\n",
      "9 10\n",
      "7 10\n",
      "6 10\n",
      "0 0\n",
      "6 10\n",
      "6 10\n",
      "3 10\n",
      "0 0\n",
      "0 0\n",
      "3 10\n",
      "6 10\n",
      "4 10\n",
      "8 10\n",
      "1 10\n",
      "0 0\n",
      "6 10\n",
      "2 10\n",
      "3 10\n",
      "5 10\n",
      "4 10\n",
      "10 10\n"
     ]
    }
   ],
   "source": [
    "#a is a memory of all arms played with resp. reward\n",
    "a = []\n",
    "# array of means for each play\n",
    "mean = []\n",
    "#array of each arm played \n",
    "arm = []\n",
    "\n",
    "for i in range(500):\n",
    "# for i in range(500):   \n",
    "    \n",
    "#     print(i)\n",
    "    \n",
    "    if random.random() > eps: # exploitation action\n",
    "        \n",
    "        if len(a) > 0:\n",
    "            bestArm(a)\n",
    "        \n",
    "    else: # exploration action\n",
    "        av = np.array([np.random.randint(0,(n+1)), 0]).reshape(1,2)\n",
    "        cash = reward(av[0][0])\n",
    "        arm1 = av[0][0]\n",
    "        print()\n",
    "        a.append([arm1, cash])\n",
    "        print(arm1, cash)\n",
    "        \n",
    "    # Calculate the mean reward\n",
    "    \n",
    "    # Plot the mean reward against the number of times the game is played\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have implemented the algorithm correctly, you should see that your agent learns to choose the arm which gives it the maximum average reward after several iterations of gameplay. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c99daf2df0>]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c99db4e310>]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(arm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
